% Preamble
\documentclass[11pt]{article}
\usepackage{braket}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}

\usepackage{makeidx}  % allows for indexgeneration
\usepackage{ifpdf}
\usepackage{url}


\title{Elementary Cellular Automata as Non-Cryptographic Hash Functions}
\date{May 2025}
\author{Daniel McKinley}

% Document
\begin{document}

\maketitle

\section{Introduction}

Elementary cellular automata (ECA) are 8 bit truth tables (Wolfram codes) done linearly in parallel. \cite{Wolfram}
Here a subset of 8 of the 256 ECA rules are explored as a non-cryptographic hash function using a lossy compression error-minimization function. The hash's key properties are that the codewords are unique and evenly distributed, has an inverse, and that hashed data can be operated on without inversion and without the original data.  The loops parallel the nested $2^n$ structure of the Fast Fourier Transform (FFT) and Fast Walsh-Hadamard Transform and the Hadamard parity of a codeword can be substituted in the inversion process. General algorithm outline, specific ECA rules, and aggregate properties are discussed. It is implemented in Java at \cite{mygit} and applied to bitmap images along with more images and gifs at the website.\\

\section{Main Algorithm}
\begin{center}
\includegraphics{testScreenshot}\\
Original Image\\
\includegraphics{processedDepth3}\\
Frame 3\\
\includegraphics{processedDepth6}\\
Frame 6\\
There are $2^{16}=65536$ binary 4x4 arrays\\
 Where there are $2^4$ possible $row_0$ neighborhoods for a given ECA rule\\
 Each of these 16 input-ECAoutput arrays are scored for errors by\\
\[  \sum_{r=0}^{3} \sum_{c=0}^{3} 2^r ( compressionAttempt_{r c} \oplus original_{r c}) \]\\
 The minimizing and maximizing values of all possible inputs\\
 are noted as the codeword pair of the original binary matrix\\
\includegraphics{inputGrid}\\
\includegraphics{ecaGrid}\\
\includegraphics{errorGrid}\\
\end{center}
The hash algorithm is a kind of lossy compression that operates on 4x4 wrapped cells of binary input data. Within each cell, row 0 is the input neighborhood and the ECA rule's output is calculated for rows 1,2 and 3. All 16 possible row 0 inputs are calculated and then scored where each bitwise discrepancy between the codeword's output and the input is summed with a weight of $2^row$. The input neighborhoods that minimize and maximize the error score are noted as the codeword pair for that 4x4 input and each codeword is 4 bits. Doing this procedure for all  $2^{16}=65536$ possible 4x4 input neighborhoods produces a truth table for a particular ECA rule.\\

Having produced a truth table as above for a given ECA rule, it is applied to a bitmap image as follows. For every (wrapped) (row,column) location in the input, the 4x4 local cell is the hexadecimal of that location and its right, down, and diagonal right-down neighbors $2^d$ away, $(row,column)..((row+2^d),(column+2^d))$  where d is depth of iteration. The value is replace with the respective minimizing or maximizing codewords. This comparing of neighbors of powers of 2 distance away is the same flow chart as the FFT and Fast Walsh Transform with reminimizations instead of sums and/or products at every level of recursion. These exponential neighborhood expansions guarantee the avalanche property in $log_2(size)$ time. Using the subset of ECA rules described below, there is only one set of codewords that describe it.\\
\begin{center}
Below is a Fast Walsh-AlgorithmCode.Hadamard Example \cite{enwiki:1261916659}\\
\includegraphics{FastWalshHadamard}\\
If you did the above with the powers of 2 in reverse order you would get this. \\
One axis of this algorithm, and instead of a sum term it's a rehash.\\ 
Find the codewords of the codewords, twice as far apart each time.\\
\includegraphics{AlgoStruct}\\
\end{center}

This transformation has two versions of the inverse, one is a lossy conversion from codewords back to output format and the other is an inverse of a hash iteration which is perfectly invertible. The conversion back to original output format does not generally apply when using more than a few iterations of the hash but may be useful in other application such as signal processing or edge detection. A single rule of the positive 8-tuple of the below is lossy, with an error rate of 3/8, however if the input is hashed as a set with every rule in the tuple, a voting process produces the inverse and the loss rate becomes 1-2\%. With a rectangular bitmap, the overlapping codewords for all 8 of the rules in the subset as well as the 8 max codewords together make a weighted vote on the inverse of the algorithm. Each codeword in the array produces a 4x4 ECA output that is its best guess about the information given to it. The cell's output is weighted by its relative row and added to its location's tally of votes. At the end if the sum of the weighted votes is positive then it becomes 0, if it is negative it becomes 1. \\

This algorithm both minimizes the errorScore in a lossy compression, and maximize the errorScore. There are dual min-max versions of most variables in the implementation. \\

\section{A subset of the 0-255 ECA}

Within the 256 ECA rules, there are 8 [0,15,51,85,170,204,240,255] that have the properties of unique codewords for any given input as well as a perfectly even distribution of codewords across the 65536 neighborhoods in the truth table. 0 and 255 are included because in the 4 rows of the output matrix, 1 is neighborhood input and 3 are output, and so still produce an errorScore and therefore a unique solution. These properties apply to these rules with both errorScore minimization and errorScore maximization. This list shares most of its members with the XOR-additive list. \cite{xorAdditive}.\\

\section{Hash properties}

These properties apply to the 8-subset described above unless otherwise specified.\\

This algorithm does not produce collisions. On their own codeword tuples are not distinct for a given bitmap even if an individual codeword is; however wrapping the codewords over themselves the tuple becomes unique; the rare artificially produced small neighborhood collisions disappeared when the same sample was wrapped, eliminating the possibility of any collision. An extra wrap loop on each cell adds factor of 16 in both time and memory for an absolute no-collision set. Without that loop, random testing of collisions produced an one in a hundred trillion on a 5x5 matrix that disappeared when the neighborhood was wrapped. Small scale local collisions may be a possibility, large scale collisions are unlikely, and optionally no collisions.\\

The 8 Wolfram codes have an even distribution of codewords. Each 0..15 codeword is used 4096 times, distributed relatively evenly across the 65536 possible binary 4x4 cells.\\

Hashed input can be operated on without the original input and without inverting it. Because the ECA rules in the 8-subset are linear, if you take any 2 of 16 codeword-generated 4x4 cells and OR them together and reminimize, the result is the same as the original two codewords ANDed together.  This shift of logic operations within a hash is uniform within an ECA rule hash and extends to any depth of iteration the hash algorithm transforms not only the input data but also the relative logic gate. For example if I want to retroactively apply a bitmask to a hashed image or IP address without the original image and without inversion, hash the bitmask and lookup the appropriate logic gate tranformation between the hashed image and the hashed bitmask.\\

\section{Bitmap Implementation and Testing}

The algorithm is prototyped on 4-byte, 3-byte, and 2-byte RGB *.bmp bitmap files, including the first several images which come from a Linux screenshot then into *.bmp by GIMP. The program converts the image's raster in a rectangular hexadecimal array. Animated *.gif files are available at my website. You can see the areas of the image cloning itself 2 by 2 and slowly dissolving into avalanche territory that eventually just looks like noise everywhere. The current image was chosen because you can clearly see parts of the image doubling itself as the avalanche property slowly takes over. The code works with any size bitmap. Future work on the project would include other image formats as it is a color code hexadecimal conversion. \\

Testing

\section{Other rules, shapes, and sizes}

The weight in the errorScore sum can be $2^{row}$ or $2^{column}$, both produce the same set of 8 tuples with the unique solution and even distribution properties, though other ECA rules' properties don't necessarily carry over. This transposition and reflections across an axis may be enough to produce unique tuples without the loop, however checking this is ongoing.\\

The size of the input array can be easily be any power of 2 squared. Calculation of Wolfram code lengths of $2^{16}$ are acceptable, lengths of $2^{64}$ would be challenging. At size 8 there are $2^8=256$ possible codewords that grow exponentially but not doubly exponentially because the codeword is only the first row of 8, which means that while you can't calculate the whole truth table at once you can calculate the codewords on the spot. The 8 tuple's uniqueness and distribution properties may or may not apply at size 8, it may not; it is only exhaustively tested for size 4.\\

Out of the other 0-255 ECA rules, some do better than these particular 8 at lossy compression, losing only 3/16 bits instead of 6/16 bits with these 8. In particular the Pascal rules 90, 165, 102, 153, 105, 150 rank near the top, connected to several these 8 via the property of XOR-additiveness (cite wolfram atlas). However none outside of these 8 have unique solutions or even distributions in either row or column weighted, maxxed or minned.\\

\section{Rule 150}

If while running this algorithm for rule 150 you produce a heat map of the errors in reconstructing the lossy compression, you get this. To seven binary digits, five past the decimal place, Phi and Pi show up in these ratios. Seven binary digits is a 2\% error rate. Seven digits is enough for an ASCII operation. Precision to seven decimal places shows up here and in reconstructing the 8-tuples above. If you label a five point star and proceed backwards starting at 0, you get \{3,1,4,2,0\} which is roughly the same seven digit precision. Some of those mentioned here are seven binary digits some are accurate to nine. While seven digits by itself is not impressive, 7 digits across three constants is notable.\\

\begin{center}
\includegraphics{PiRatios}\\
\includegraphics{RatioEquations}\\
\end{center}

\bibliographystyle{plain}
\bibliography{HashBib.bib}

\end{document}